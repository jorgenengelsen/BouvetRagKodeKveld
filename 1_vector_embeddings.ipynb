{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to Vector Databases with sentence embeddings\n",
    "\n",
    "Vector databases are designed to store and query high-dimensional vectors, such as embeddings generated from sentences or documents. These embeddings can capture the semantic meaning of the text and are useful for a variety of natural language processing tasks.\n",
    "\n",
    "## What are vector embeddings?\n",
    "\n",
    "vector embeddings are fixed-size vectors that represent sentences in a high-dimensional space. These embeddings are generated such that semantically similar sentences are close to each other in the embedding space.\n",
    "\n",
    "## Using the `sentence-transformers` Library\n",
    "\n",
    "The `sentence-transformers` library allows us to easily generate sentence embeddings with pre-trained models. These models are trained on large text corpora and can generate high-quality embeddings for a wide range of sentences. For more info: https://sbert.net/index.html \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8d77380d0744d17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%html \n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/klTvEwg3oJ4?si=TEc_pmoM5I3TWCwn\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27bf5739bf44195a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "class SimpleTextEncoder:\n",
    "    \"\"\" A naive text encoder that counts the occurrences of each letter in the text. \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def encode(self, texts: str | list[str], batch_size=None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes a list of texts to fixed-size vectors where each dimension\n",
    "        corresponds to the count of a specific letter in the text.\n",
    "        \n",
    "        :param texts: A list of strings to encode.\n",
    "        :param batch_size: Not used in this simple encoder, just for compatibility.\n",
    "        :return: A numpy array of shape (len(texts), len(self.dimensions))\n",
    "        \"\"\"\n",
    "        # If a single string is provided, wrap it in a list\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "            \n",
    "        # Define the columns to be the alphabet (a-z)\n",
    "        dimensions = string.ascii_lowercase    \n",
    "        \n",
    "        # Initialize an array to hold the encodings\n",
    "        encodings = np.zeros((len(texts), len(dimensions)), dtype=float)\n",
    "        \n",
    "        # Encode each text\n",
    "        for i, text in enumerate(texts):\n",
    "            # Normalize the text to lower case and filter out non-alphabetic characters\n",
    "            text = text.lower()\n",
    "            text = filter(str.isalpha, text)\n",
    "\n",
    "            # Count the occurrences of each letter\n",
    "            letter_counts = Counter(text)\n",
    "            \n",
    "            # Fill the encoding array with the counts for each letter\n",
    "            for j, letter in enumerate(dimensions):\n",
    "                encodings[i, j] = float(letter_counts[letter])\n",
    "        \n",
    "        return encodings\n",
    "\n",
    "homemade_model = SimpleTextEncoder()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fcd36d8bfdde8e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the model\n",
    "pretrained_model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bdd1c50032d4f30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# select model\n",
    "model = pretrained_model\n",
    "#model = homemade_model\n",
    "\n",
    "# Our list of sentences\n",
    "sentences = [\n",
    "    \"A man is eating food.\",\n",
    "    \"A man is eating a piece of bread.\",\n",
    "    \"The girl is carrying a baby.\",\n",
    "    \"A man is riding a horse.\",\n",
    "    \"A woman is playing violin.\",\n",
    "    \"Two men pushed carts through the woods.\",\n",
    "    \"A man is riding a white horse on an enclosed ground.\",\n",
    "    \"A monkey is playing drums.\",\n",
    "    \"Someone in a gorilla costume is playing a set of drums.\"\n",
    "]\n",
    "\n",
    "# Generate the sentence embeddings\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Print the embeddings for the first sentence\n",
    "print(\"Dimensions in embedding:\", len(embeddings[0]))\n",
    "\n",
    "print(sentences[0])\n",
    "print(\"Start of embedding of the first sentence:\", embeddings[0][0:5])\n",
    "\n",
    "if isinstance(model, SimpleTextEncoder):\n",
    "    print(f\"A: {embeddings[0][0]}\")\n",
    "    print(f\"B: {embeddings[0][1]}\") \n",
    "    print(f\"C: {embeddings[0][2]}\") \n",
    "    print(f\"D: {embeddings[0][3]}\") \n",
    "    print(f\"E: {embeddings[0][4]}\") \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bc9bd0f82ec1078"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from ipywidgets import interactive, HBox, VBox\n",
    "\n",
    "# Assuming 'embeddings' is a 2D numpy array and 'sentences' is the list of sentences\n",
    "# We will create a function to plot any two dimensions\n",
    "\n",
    "def plot_embeddings(dim1, dim2):\n",
    "    # Create a DataFrame for the plot\n",
    "    df = pd.DataFrame(embeddings[:, [dim1, dim2]], columns=['Dim 1', 'Dim 2'])\n",
    "    df['Sentence'] = sentences\n",
    "    \n",
    "    # Create the 2D plot\n",
    "    fig = px.scatter(df, x='Dim 1', y='Dim 2', text='Sentence', \n",
    "                     title=f'Sentence Embeddings Visualization (Dimensions {dim1+1} vs {dim2+1})')\n",
    "    fig.update_traces(textposition='top center')\n",
    "    fig.update_layout(transition_duration=500)\n",
    "    fig.show()\n",
    "\n",
    "# Create a slider for each dimension\n",
    "slider_1 = interactive(plot_embeddings, dim1=(0, 9), dim2=(0, 9))\n",
    "\n",
    "# Display the sliders and plot\n",
    "display(VBox([HBox(slider_1.children[:-1]), slider_1.children[-1]]))\n",
    "slider_1.update()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7cdea38f3272fd1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "# Compute cosine similarity between all pairs\n",
    "cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "# Add all pairs to a list with their cosine similarity score\n",
    "all_sentence_combinations = []\n",
    "for i in range(len(cos_sim) - 1):\n",
    "    for j in range(i + 1, len(cos_sim)):\n",
    "        all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "# Sort list by the highest cosine similarity score\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print(\"Top-5 most similar pairs:\")\n",
    "for score, i, j in all_sentence_combinations[0:5]:\n",
    "    print(\"{} \\t {} \\t {:.4f}\".format(sentences[i], sentences[j], cos_sim[i][j]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e9d02af68bd4501"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "query = \"A man is eating food.\" # Perfect match\n",
    "query = \"jam\" # Rely on models pre-training\n",
    "\n",
    "# Encode the query sentence\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "# Compute cosine similarity between the query embedding and all sentence embeddings\n",
    "cos_similarities = util.cos_sim(query_embedding, embeddings)[0]\n",
    "\n",
    "# Find the index of the sentence with the highest similarity\n",
    "closest_idx = cos_similarities.argmax()\n",
    "\n",
    "# Return the closest sentence and its similarity score\n",
    "closest_sentence = sentences[closest_idx]\n",
    "similarity_score = cos_similarities[closest_idx].item()\n",
    "\n",
    "print(\"Closest sentence:\", closest_sentence)\n",
    "print(\"Similarity score:\", similarity_score)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcd397f5f6b33849"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 1: Implement your own embedding model, bonus points for using chatGPT! (15 minutes)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1ed9f5dbd9decda"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
